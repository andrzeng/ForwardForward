{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torchvision\n",
    "import random\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"data\", download=True, \n",
    "                               train=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,)),\n",
    "     transforms.Resize((14,14))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(root=\"data\", download=True,train=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.Resize((14,14))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ffmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(196,40)\n",
    "        self.layernorm1 = nn.LayerNorm(40, elementwise_affine=False)\n",
    "\n",
    "        self.linear2 = nn.Linear(40,40)\n",
    "        self.layernorm2 = nn.LayerNorm(40, elementwise_affine=False)\n",
    "        \n",
    "        self.linear3 = nn.Linear(40,40)\n",
    "        self.layernorm3 = nn.LayerNorm(40, elementwise_affine=False)\n",
    "        \n",
    "        self.linear4 = nn.Linear(40,40)\n",
    "        self.layernorm4 = nn.LayerNorm(40, elementwise_affine=False)\n",
    "        \n",
    "\n",
    "    def forward(self, X, \n",
    "                #valency #-1 or 0 or 1 depending on goodness or badness of data or the other way around idk\n",
    "                ):\n",
    "        X = X.flatten(start_dim=1)\n",
    "        \n",
    "        X = self.linear1(X)\n",
    "        X = self.layernorm1(X)\n",
    "        X = torch.relu(X)\n",
    "        ssqa1 = torch.sum(X**2)\n",
    "        X = X.detach() # Detaching X resets the tree that autograd uses to compute gradients. Thus parameter updates will not proceed further down than the local layer\n",
    "\n",
    "        X = self.linear2(X)\n",
    "        X = self.layernorm2(X)\n",
    "        X = torch.relu(X)\n",
    "        ssqa2 = torch.sum(X**2)\n",
    "        X = X.detach()\n",
    "        \n",
    "        X = self.linear3(X)\n",
    "        X = self.layernorm3(X)\n",
    "        X = torch.relu(X)\n",
    "        ssqa3 = torch.sum(X**2)\n",
    "        X = X.detach()\n",
    "\n",
    "        X = self.linear4(X)\n",
    "        X = self.layernorm4(X)\n",
    "        X = torch.relu(X)\n",
    "        ssqa4 = torch.sum(X**2)\n",
    "        \n",
    "        \n",
    "        return X, (ssqa1, ssqa2, ssqa3, ssqa4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0\n",
      "Error rate is 0.929070929070929\n",
      "starting epoch 1\n",
      "Error rate is 0.5944055944055944\n",
      "starting epoch 2\n",
      "Error rate is 0.3546453546453546\n",
      "starting epoch 3\n",
      "Error rate is 0.3356643356643357\n",
      "starting epoch 4\n",
      "Error rate is 0.3646353646353646\n",
      "starting epoch 5\n",
      "Error rate is 0.3016983016983017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m   out, ssqs \u001b[39m=\u001b[39m model(image)\n\u001b[0;32m     40\u001b[0m   \u001b[39mfor\u001b[39;00m ssq \u001b[39min\u001b[39;00m ssqs:\n\u001b[1;32m---> 41\u001b[0m       (ssq \u001b[39m*\u001b[39;49m valency)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     42\u001b[0m  \u001b[39m# print(valency, ssqs)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[39m#  print(model.linear1.weight.grad)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ffmodel()\n",
    "model.load_state_dict(torch.load(\"good_weights.pt\"))\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "arr = np.zeros((10,9))\n",
    "for row_index in range(10):\n",
    "    arr[row_index] = [i for i in range(10) if i != row_index]\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(f'starting epoch {epoch}')\n",
    "    eval()\n",
    "    for index, (image, label) in enumerate(train_dataset):\n",
    "        \n",
    "        image[:,0,:10] = torch.zeros_like(image[:,0,:10])\n",
    "\n",
    "        if(random.uniform(0,1) < 0.5): # Positive example\n",
    "            valency = -1\n",
    "            image[:,0,label] = 1\n",
    "\n",
    "        else: # Negative example\n",
    "            valency = 1\n",
    "            image[:,0, random.choice(arr[label]).astype(np.int16)] = 1\n",
    "        if(index == 10000):\n",
    "            break\n",
    "        \n",
    "       \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, ssqs = model(image)\n",
    "        for ssq in ssqs:\n",
    "            (ssq * valency).backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "def eval():\n",
    "    total = 0\n",
    "    incorrect = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for index, (image, label) in enumerate(test_dataset):\n",
    "            \n",
    "            activities_list = []\n",
    "            for c in range(10):\n",
    "                image[:,0,:10] = torch.zeros_like(image[:,0,:10])\n",
    "                image[:,0,c] = 1\n",
    "\n",
    "                out, ssqs = model(image)\n",
    "                activities_list.append((c, ssqs[1] + ssqs[2] + ssqs[3]))\n",
    "            predicted_label = max(activities_list, key=lambda tup: tup[1])[0]\n",
    "            \n",
    "            total += 1\n",
    "            if(predicted_label != label):\n",
    "                incorrect += 1\n",
    "                #print(predicted_label, label)\n",
    "\n",
    "            if(index == 1000):\n",
    "                break\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Error rate is {incorrect/total}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
